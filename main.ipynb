{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GE2340 Course Group Project Summary\n",
    "##### In this project, our primary objective is to evaluate and compare the performance of the Proximal Policy Optimization (PPO) algorithm within the LunarLander environment provided by the Gymnasium library. We selected the LunarLander environment due to its comparatively low computational demands, especially when contrasted with more resource-intensive environments such as Atari games available within the same framework. This choice allows us to conduct experiments efficiently without the need for extensive computational resources.\n",
    "\n",
    "##### To facilitate the implementation of the PPO algorithm and streamline the process of adjusting hyperparameters, we utilized the Stable Baselines 3 (SB3) framework. SB3 offers a comprehensive collection of pre-implemented reinforcement learning algorithms, which simplifies our experimentation process and reduces the time required for development. By leveraging SB3, we can focus on analyzing the algorithm's performance and make informed adjustments to optimize outcomes effectively.\n",
    "\n",
    "-- Code comments are purely for team coordination and following work, you can ignore them"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Download and Importing all dependencies",
   "id": "985db9bd3fef3b78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T11:32:54.984522Z",
     "start_time": "2024-11-14T11:32:53.295880Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install gymnasium\\[box2d\\] swig stable-baselines3 tensorboard",
   "id": "d107aeb76fdb54b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /opt/anaconda3/lib/python3.12/site-packages (4.2.1.post0)\r\n",
      "Requirement already satisfied: ale-py in /opt/anaconda3/lib/python3.12/site-packages (0.10.1)\r\n",
      "Requirement already satisfied: stable-baselines3 in /opt/anaconda3/lib/python3.12/site-packages (2.4.0a11)\r\n",
      "Requirement already satisfied: tensorboard in /opt/anaconda3/lib/python3.12/site-packages (2.18.0)\r\n",
      "Requirement already satisfied: gymnasium[box2d] in /opt/anaconda3/lib/python3.12/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium[box2d]) (1.26.4)\r\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium[box2d]) (2.2.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium[box2d]) (4.11.0)\r\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium[box2d]) (0.0.4)\r\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium[box2d]) (2.3.5)\r\n",
      "Requirement already satisfied: pygame>=2.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from gymnasium[box2d]) (2.6.1)\r\n",
      "Requirement already satisfied: torch>=1.13 in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3) (2.5.1)\r\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3) (2.2.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.12/site-packages (from stable-baselines3) (3.9.2)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (2.1.0)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.67.1)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.4.1)\r\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (23.2)\r\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.20.3)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (75.3.0)\r\n",
      "Requirement already satisfied: six>1.9 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (1.16.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard) (3.0.3)\r\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (3.13.1)\r\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (2024.3.1)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13->stable-baselines3) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (4.51.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (1.4.4)\r\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (10.3.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->stable-baselines3) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->stable-baselines3) (2023.3)\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:14:42.027846Z",
     "start_time": "2024-11-14T14:14:42.011622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gymnasium as gym # To use the environment\n",
    "\n",
    "from stable_baselines3 import PPO # These are the algorithms from SB3\n",
    "from stable_baselines3.common.evaluation import evaluate_policy # This is needed for evaluation\n",
    "from stable_baselines3.common.monitor import Monitor # for evaluation purposes too\n",
    "\n",
    "# Following two imports are to vectorize multiple environments that are running in parallel\n",
    "# Basically, to speed up the process of training\n",
    "from stable_baselines3.common.env_util import make_vec_env \n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "import pandas as pd # This is to save results in a tabular format"
   ],
   "id": "5538ebc8a65d11e",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### To view the statistics of the algorithms, we will use Tensorboard.",
   "id": "80ae7aacfbd963f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:08:39.589944Z",
     "start_time": "2024-11-14T12:08:39.578240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is a log path to store the statistics of every training.\n",
    "log_path = 'data/logs'"
   ],
   "id": "1d728c499985d623",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:45:39.587985Z",
     "start_time": "2024-11-14T12:45:37.501340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Entering this command in terminal is preferred (without the exclamation mark in the beginning)\n",
    "# As it is a bit troublesome inside the jupyter notebook\n",
    "\n",
    "!tensorboard --logdir 'data/logs'\n",
    "\n",
    "# Copy & paste this to the terminal:\n",
    "# tensorboard --logdir 'data/logs'"
   ],
   "id": "4a0471cea0e2b6e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/compat/__init__.py\", line 42, in tf\r\n",
      "    from tensorboard.compat import notf  # noqa: F401\r\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "ImportError: cannot import name 'notf' from 'tensorboard.compat' (/opt/anaconda3/lib/python3.12/site-packages/tensorboard/compat/__init__.py)\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/opt/anaconda3/bin/tensorboard\", line 8, in <module>\r\n",
      "    sys.exit(run_main())\r\n",
      "             ^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/main.py\", line 38, in run_main\r\n",
      "    main_lib.global_init()\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/main_lib.py\", line 50, in global_init\r\n",
      "    if getattr(tf, \"__version__\", \"stub\") == \"stub\":\r\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/lazy.py\", line 65, in __getattr__\r\n",
      "    return getattr(load_once(self), attr_name)\r\n",
      "                   ^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/lazy.py\", line 97, in wrapper\r\n",
      "    cache[arg] = f(arg)\r\n",
      "                 ^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/lazy.py\", line 50, in load_once\r\n",
      "    module = load_fn()\r\n",
      "             ^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorboard/compat/__init__.py\", line 45, in tf\r\n",
      "    import tensorflow\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorflow/__init__.py\", line 40, in <module>\r\n",
      "    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow  # pylint: disable=unused-import\r\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 34, in <module>\r\n",
      "    self_check.preload_check()\r\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tensorflow/python/platform/self_check.py\", line 63, in preload_check\r\n",
      "    from tensorflow.python.platform import _pywrap_cpu_feature_guard\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## First let's understand an environment, action space and the observation space\n",
    "\n",
    "All of these can be found in the documentation of the environment: https://gymnasium.farama.org/environments/box2d/lunar_lander/"
   ],
   "id": "9ced7ac74682f53e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T11:34:37.220331Z",
     "start_time": "2024-11-14T11:34:36.671181Z"
    }
   },
   "cell_type": "code",
   "source": "env = gym.make('LunarLander-v2') # Initializing the LunarLander environment",
   "id": "aa98e41a197c7eef",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T11:35:13.271557Z",
     "start_time": "2024-11-14T11:35:13.257873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The action space is discrete with 4 actions available:\n",
    "# 0 - do nothing\n",
    "# 1 - fire left orientation engine\n",
    "# 2 - fire main engine\n",
    "# 3 - fire right orientation engine\n",
    "\n",
    "env.action_space"
   ],
   "id": "6b3c19884503d92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T11:36:33.626863Z",
     "start_time": "2024-11-14T11:36:33.617565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The observation space is box (8-dimensional vector), meaning that you receive values bounded within specific values.\n",
    "# The first list below contains lower bounds, the second represent the upper bounds for each value within a list\n",
    "# The values themselves represent:\n",
    "# x and y coordinates of the spacecraft\n",
    "# its linear velocities in x and y\n",
    "# its angle and angular velocity\n",
    "# and two boolean values 0 or 1 representing whether each leg is in contact with the ground or not\n",
    "\n",
    "env.observation_space"
   ],
   "id": "9d9615b7e00f7985",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.5       -1.5       -5.        -5.        -3.1415927 -5.\n",
       " -0.        -0.       ], [1.5       1.5       5.        5.        3.1415927 5.        1.\n",
       " 1.       ], (8,), float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T11:42:35.229637Z",
     "start_time": "2024-11-14T11:42:35.185985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This is an example of one observation that an agent receives\n",
    "\n",
    "env.reset()"
   ],
   "id": "8bc9143d204e2e20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.3456345e-03,  1.4103701e+00,  1.3627613e-01, -2.4438359e-02,\n",
       "        -1.5524103e-03, -3.0868551e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### For administrative use only, ignore this.\n",
    "\n",
    "Just functions for training the model, visualizing it, recording the gameplay, evaluating, etc.\n",
    "They are going to be broken down later, while explaining the baseline model"
   ],
   "id": "21bb463bd55c6487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:41:46.101925Z",
     "start_time": "2024-11-14T14:41:46.094518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_agent(algo, timesteps, log_name, policy=\"MlpPolicy\", log_path=\"data/logs\", lr = 0.0003):\n",
    "    env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "    model = algo(policy, env, learning_rate=lr, device=\"cpu\", tensorboard_log=log_path)\n",
    "    model.learn(total_timesteps=timesteps, tb_log_name=log_name)\n",
    "    \n",
    "    return model"
   ],
   "id": "9e79a96ce565bd7b",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:34:48.788320Z",
     "start_time": "2024-11-14T13:34:48.780760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_agent(model_path, episodes=5, algo=PPO):\n",
    "    model = algo.load(model_path)\n",
    "    env = gym.make('LunarLander-v2', render_mode='human')\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        \n",
    "        while not terminated and not truncated:\n",
    "            action, _state = model.predict(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            env.render()\n",
    "        \n",
    "        print(f'Episode: {episode + 1}, Reward: {episode_reward}')\n",
    "    \n",
    "    env.close()"
   ],
   "id": "ca401dbdb861c536",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:13:51.210239Z",
     "start_time": "2024-11-14T14:13:51.202367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_rl_model(algo, model_path, n_eval_episodes=100, deterministic=True):\n",
    "    env = Monitor(gym.make('LunarLander-v2'))\n",
    "    model = algo.load(model_path)\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=n_eval_episodes, deterministic=deterministic)\n",
    "    \n",
    "    print(f\"{algo.__name__} - Mean Reward: {mean_reward} +/- {std_reward}\")\n",
    "    \n",
    "    return mean_reward, std_reward"
   ],
   "id": "49044044b6c919ef",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:19:41.260359Z",
     "start_time": "2024-11-14T14:19:41.235633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_evaluation(name, timesteps, mean_reward=mean_reward, std_reward=std_reward, results_df=results):\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Algorithm': [name],\n",
    "        'Mean Reward': [mean_reward],\n",
    "        'Standard Deviation': [std_reward],\n",
    "        'Timesteps': [timesteps]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "    return results_df"
   ],
   "id": "c1f993545e54c51f",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:01:26.967153Z",
     "start_time": "2024-11-14T15:01:26.961448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_evaluation_lr(name, lr, results_df, mean_reward=mean_reward, std_reward=std_reward):\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Algorithm': [name],\n",
    "        'Mean Reward': [mean_reward],\n",
    "        'Standard Deviation': [std_reward],\n",
    "        'Learning Rate': [lr]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "    return results_df"
   ],
   "id": "f4301df466d29227",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:42:53.487540Z",
     "start_time": "2024-11-14T15:42:53.480782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log_evaluation_arch(name, arch, results_df, mean_reward=mean_reward, std_reward=std_reward):\n",
    "    temp_df = pd.DataFrame({\n",
    "        'Algorithm': [name],\n",
    "        'Mean Reward': [mean_reward],\n",
    "        'Standard Deviation': [std_reward],\n",
    "        'Architecture': [arch]\n",
    "    })\n",
    "    results_df = pd.concat([results_df, temp_df], ignore_index=True)\n",
    "    return results_df"
   ],
   "id": "786814ff079214c2",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Let's initialize the environment and visualize the performance of an agent that performs random actions in the environment\n",
    "\n",
    "#### For local usage only, since Gymnasium code visualizes the game in an external window of PyGame"
   ],
   "id": "cb68c2cf7d7b91f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:32:10.201266Z",
     "start_time": "2024-11-16T12:31:51.739705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='human') # this render mode renders the environment using PyGame to visualize the gameplay\n",
    "\n",
    "episodes = 5 # episode is a single run of the game\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset() # resetting the environment is necessary\n",
    "    episode_reward = 0 # cumulative reward for the whole episode\n",
    "    terminated = False # for the termination of the episode because of the game rules (e.g. the lander crushes)\n",
    "    truncated = False # to finish the episode if the time limit is reached\n",
    "\n",
    "    while not terminated and not truncated: # if any of those flags becomes true, then the episode will finish; to avoid the infinite loop\n",
    "        action = env.action_space.sample() # take a random action from the action_space\n",
    "        obs, reward, terminated, truncated, info = env.step(action) # env.step produces the reward and observation; that will be the agent input\n",
    "        episode_reward += reward # sum up rewards for every step\n",
    "        env.render() # to visualize the environment\n",
    "\n",
    "    print(f'Episode: {episode + 1}, reward: {episode_reward}') # this prints the episode number and reward\n",
    "\n",
    "env.close()\n"
   ],
   "id": "b7eb94eca72225b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, reward: -111.10050402559793\n",
      "Episode: 2, reward: -113.05190105473527\n",
      "Episode: 3, reward: -179.09979614175077\n",
      "Episode: 4, reward: -211.21555214185832\n",
      "Episode: 5, reward: -174.2443513547509\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### We plan to conduct several tests to gain insights into the PPO algorithm. \n",
    "### But let's make a default model to use as a baseline to other models"
   ],
   "id": "314f8e6a55a04eb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:12:01.152578Z",
     "start_time": "2024-11-14T12:12:01.078682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# To check the standard parameters for the PPO algorithm\n",
    "\n",
    "PPO??\n",
    "\n",
    "# PPO(\n",
    "#     policy: Union[str, Type[stable_baselines3.common.policies.ActorCriticPolicy]],\n",
    "#     env: Union[gymnasium.core.Env, ForwardRef('VecEnv'), str],\n",
    "#     learning_rate: Union[float, Callable[[float], float]] = 0.0003,\n",
    "#     n_steps: int = 2048,\n",
    "#     batch_size: int = 64,\n",
    "#     n_epochs: int = 10,\n",
    "#     gamma: float = 0.99,\n",
    "#     gae_lambda: float = 0.95,\n",
    "#     clip_range: Union[float, Callable[[float], float]] = 0.2,\n",
    "#     clip_range_vf: Union[NoneType, float, Callable[[float], float]] = None,\n",
    "#     normalize_advantage: bool = True,\n",
    "#     ent_coef: float = 0.0,\n",
    "#     vf_coef: float = 0.5,\n",
    "#     max_grad_norm: float = 0.5,\n",
    "#     use_sde: bool = False,\n",
    "#     sde_sample_freq: int = -1,\n",
    "#     rollout_buffer_class: Optional[Type[stable_baselines3.common.buffers.RolloutBuffer]] = None,\n",
    "#     rollout_buffer_kwargs: Optional[Dict[str, Any]] = None,\n",
    "#     target_kl: Optional[float] = None,\n",
    "#     stats_window_size: int = 100,\n",
    "#     tensorboard_log: Optional[str] = None,\n",
    "#     policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "#     verbose: int = 0,\n",
    "#     seed: Optional[int] = None,\n",
    "#     device: Union[torch.device, str] = 'auto',\n",
    "#     _init_setup_model: bool = True,\n",
    "# )"
   ],
   "id": "6283b31bdaccc556",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:45:13.564160Z",
     "start_time": "2024-11-14T12:34:31.722523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This will be our stock model, training it with default parameters mentioned above on 1mil timesteps\n",
    "\n",
    "# Training here is done using 8 cores of the CPU in parallel.\n",
    "# That's why we need SubprocVecEnv, make_vec_env\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "# The CPU is used here, since PPO was meant to be used on CPU primarily (SB3 docs)\n",
    "model = PPO(\"MlpPolicy\", env, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1_000_000,  tb_log_name='PPO_default')"
   ],
   "id": "f69625480d0f25b5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x16c79d190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T20:13:28.967249Z",
     "start_time": "2024-11-14T20:13:28.960511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SB3 allows us to use MLPs to implement the Actor-Critic Policy\n",
    "# The MLP architecture consists of 2 networks for both the value function (critic) and the policy (actor)\n",
    "# Each of them consist of 2-fully connected hidden layers with 64 units per layer\n",
    "# Tanh is used as the activation function for the hidden layers\n",
    "# The model also involves the Feature Extractor\n",
    "# Read more about it here: https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html#sb3-policy\n",
    "\n",
    "model.policy"
   ],
   "id": "dfd5b9bab239c944",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=4, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 175
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:45:39.614435Z",
     "start_time": "2024-11-14T12:45:39.595302Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_default')",
   "id": "9bcbd0cb3814edcc",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:46:38.544835Z",
     "start_time": "2024-11-14T12:46:38.539577Z"
    }
   },
   "cell_type": "code",
   "source": "del model # for administrative use only",
   "id": "2d3794f7d07a0518",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Visualizing the model's behaviour",
   "id": "7d770be4514b0bc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T12:51:57.093873Z",
     "start_time": "2024-11-14T12:51:07.989977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = PPO.load('models/PPO_default')\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='human')\n",
    "\n",
    "episodes = 5\n",
    "\n",
    "for episode in range(episodes):\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not terminated and not truncated:\n",
    "        action, _state = model.predict(obs) #\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "        \n",
    "    print(f'Episode: {episode + 1}, reward: {episode_reward}')\n",
    "\n",
    "env.close()"
   ],
   "id": "f270ffd8a30fc28d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, reward: 277.15495825688726\n",
      "Episode: 2, reward: 291.3077068253734\n",
      "Episode: 3, reward: 270.0817612896867\n",
      "Episode: 4, reward: 272.894404198532\n",
      "Episode: 5, reward: 283.8284480196621\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Saving the mp4 files of the gameplay of an agent\n",
    "\n",
    "# Check this later bro"
   ],
   "id": "565500e989fb7c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Evaluating this model over 100 episodes",
   "id": "793cd6d382b85f3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:58:20.204133Z",
     "start_time": "2024-11-17T06:58:14.010234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# No rendering here is needed\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "model = PPO.load('models/PPO_default')\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True)\n",
    "\n",
    "print(f\"PPO - Mean Reward: {mean_reward} +/- {std_reward}\")"
   ],
   "id": "b64ac55de90a701e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 261.0436890006603 +/- 41.42890211445548\n"
     ]
    }
   ],
   "execution_count": 243
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:07:58.630756Z",
     "start_time": "2024-11-14T13:07:58.615753Z"
    }
   },
   "cell_type": "code",
   "source": "results = pd.DataFrame(columns=['Algorithm', 'Mean Reward', 'Standard Deviation', 'Timesteps'])",
   "id": "22a9d88444c53daa",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:08:00.091191Z",
     "start_time": "2024-11-14T13:08:00.085333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "name = 'PPO_default'\n",
    "\n",
    "temp_df = pd.DataFrame({\n",
    "    'Algorithm': [name],\n",
    "    'Mean Reward': [mean_reward],\n",
    "    'Standard Deviation': [std_reward],\n",
    "    'Timesteps': 1_000_000})\n",
    "\n",
    "results = pd.concat([results, temp_df])"
   ],
   "id": "2a43aba64357d98a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/mj7q028j0xxgpkb_f6mxv7sc0000gp/T/ipykernel_4116/283692655.py:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, temp_df])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Firstly, we want to examine how the performance changes based on the number of timesteps the agent is trained for.\n",
    "\n",
    "Training the model for 500k timesteps, reducing the baseline amount twice"
   ],
   "id": "457b4aed210398da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:17:26.801133Z",
     "start_time": "2024-11-14T13:11:23.388939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=500_000,  tb_log_name='PPO_500k_timesteps')"
   ],
   "id": "4b341ed9b8c4d9ce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x32019b470>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T13:32:14.864091Z",
     "start_time": "2024-11-14T13:32:14.835894Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_500k_timesteps')",
   "id": "f6c9e4f78f6acae5",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "del model # for administrative use",
   "id": "f061f1aabbdb6380"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Visualizing the model",
   "id": "f4166e6406d78735"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T10:11:11.545405Z",
     "start_time": "2024-11-17T10:10:09.718341Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_500k_timesteps', episodes=5)",
   "id": "e30bd808151f444c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 258.1017631212074\n",
      "Episode: 2, Reward: 266.22540669030803\n",
      "Episode: 3, Reward: 282.04932361448215\n",
      "Episode: 4, Reward: 247.1627704224718\n",
      "Episode: 5, Reward: 267.9915426733965\n"
     ]
    }
   ],
   "execution_count": 268
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Recording",
   "id": "5c175218ecfb9152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e414a5950a556fef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluating and logging the evaluation",
   "id": "27c6d2d044f2ceb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:35:45.704835Z",
     "start_time": "2024-11-14T14:35:38.179977Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_500k_timesteps')",
   "id": "807f616670a41a05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 226.76535850999997 +/- 64.11821598336066\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:35:48.270321Z",
     "start_time": "2024-11-14T14:35:48.267630Z"
    }
   },
   "cell_type": "code",
   "source": "print(mean_reward, std_reward)",
   "id": "2b97577f29a9c970",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226.76535850999997 64.11821598336066\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:35:50.375752Z",
     "start_time": "2024-11-14T14:35:50.370593Z"
    }
   },
   "cell_type": "code",
   "source": "results = log_evaluation(name='PPO_500k', timesteps=500_000, mean_reward=mean_reward, std_reward=std_reward)",
   "id": "1711ec3bb2aa7939",
   "outputs": [],
   "execution_count": 118
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Let's increase the amount of timesteps twice:",
   "id": "c8d98dcf0e6e7ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:07:34.733597Z",
     "start_time": "2024-11-14T13:46:04.057842Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_agent(algo=PPO, timesteps=2_000_000, log_name='PPO_2mil_timesteps')",
   "id": "9d8a6f07a9c45674",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:09:12.078331Z",
     "start_time": "2024-11-14T14:09:12.020364Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_2mil_timesteps')",
   "id": "2c91606fd7f3fad7",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:37:23.764444Z",
     "start_time": "2024-11-14T14:37:18.209684Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_2mil_timesteps')",
   "id": "23cc61d36ba9f04f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 267.35464206 +/- 22.856307370573735\n"
     ]
    }
   ],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:38:43.174697Z",
     "start_time": "2024-11-14T14:38:43.165789Z"
    }
   },
   "cell_type": "code",
   "source": "results = log_evaluation(name='PPO_2mil', timesteps=2_000_000, mean_reward=mean_reward, std_reward=std_reward, results_df=results)",
   "id": "3e4aa90ab7a4a2d4",
   "outputs": [],
   "execution_count": 130
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:03:21.000400Z",
     "start_time": "2024-11-17T07:03:20.988248Z"
    }
   },
   "cell_type": "code",
   "source": "results",
   "id": "25e6e79a873a0ad8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Algorithm  Mean Reward  Standard Deviation Timesteps\n",
       "0  PPO_default   261.043689           41.428902   1000000\n",
       "1     PPO_500k   226.765359           64.118216    500000\n",
       "2     PPO_2mil   267.354642           22.856307   2000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Timesteps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PPO_default</td>\n",
       "      <td>261.043689</td>\n",
       "      <td>41.428902</td>\n",
       "      <td>1000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PPO_500k</td>\n",
       "      <td>226.765359</td>\n",
       "      <td>64.118216</td>\n",
       "      <td>500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PPO_2mil</td>\n",
       "      <td>267.354642</td>\n",
       "      <td>22.856307</td>\n",
       "      <td>2000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 246
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A second experiment is to change the learning rate of the PPO algorithm\n",
    "The default learning rate is 0.0003, let's increase it 10 times"
   ],
   "id": "d45d246ec03f497"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:54:18.293558Z",
     "start_time": "2024-11-14T14:43:53.836957Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_agent(algo=PPO, timesteps=1_000_000, log_name='PPO_lr_x10', lr=0.003)",
   "id": "a1af34b11651ebad",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:55:34.117399Z",
     "start_time": "2024-11-14T14:55:34.092069Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_lr_x10')",
   "id": "46ac5b9c63946031",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T14:55:52.523891Z",
     "start_time": "2024-11-14T14:55:47.255158Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_lr_x10')",
   "id": "3a32787ffec34d8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 235.93047351 +/- 83.43652698272338\n"
     ]
    }
   ],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:04:11.764461Z",
     "start_time": "2024-11-14T15:04:11.748488Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_lr = pd.DataFrame(columns=['Algorithm', 'Mean Reward', 'Standard Deviation', 'Learning Rate'])\n",
    "\n",
    "results_lr.loc[0] = ['PPO_default', 266.884301, 21.949487, 0.0003]"
   ],
   "id": "cffaace70294d96d",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T10:16:22.409857Z",
     "start_time": "2024-11-17T10:15:32.111057Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_lr_x10.zip')",
   "id": "63d038b7131413b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 294.2391201373126\n",
      "Episode: 2, Reward: 308.02606720363895\n",
      "Episode: 3, Reward: 281.2610729355911\n",
      "Episode: 4, Reward: 55.90130036096838\n",
      "Episode: 5, Reward: 267.85304492537927\n"
     ]
    }
   ],
   "execution_count": 269
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:05:31.708645Z",
     "start_time": "2024-11-14T15:05:31.686447Z"
    }
   },
   "cell_type": "code",
   "source": "results_lr = log_evaluation_lr(name='PPO_LR_x10', lr=0.003, results_df=results_lr, mean_reward=mean_reward, std_reward=std_reward)",
   "id": "79bf3532313d1907",
   "outputs": [],
   "execution_count": 144
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Training another model, but decreasing a learning rate 10 times",
   "id": "8e1c2f2dada0c6fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:15:30.857236Z",
     "start_time": "2024-11-14T15:09:01.242563Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_agent(algo=PPO, timesteps=1_000_000, log_name='PPO_lr_x0_1')",
   "id": "babae7014a007ee3",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:18:44.238256Z",
     "start_time": "2024-11-14T15:18:44.170709Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_lr_x0_1')",
   "id": "9d94b0306b27ef0a",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:23:36.163255Z",
     "start_time": "2024-11-14T15:23:04.217789Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_lr_x0_1', n_eval_episodes=1000)",
   "id": "c9ed18770ef33993",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 271.72410653700007 +/- 24.651670297485083\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:20:58.751406Z",
     "start_time": "2024-11-14T15:20:08.433289Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_lr_x0_1')",
   "id": "a5db3a79c091e5e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 277.1073003018405\n",
      "Episode: 2, Reward: 283.54802500154074\n",
      "Episode: 3, Reward: 271.323135518571\n",
      "Episode: 4, Reward: 264.53156146977983\n",
      "Episode: 5, Reward: 273.9498880889795\n"
     ]
    }
   ],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T15:22:03.070068Z",
     "start_time": "2024-11-14T15:22:03.051986Z"
    }
   },
   "cell_type": "code",
   "source": "results_lr = log_evaluation_lr(name='PPO_LR_x0.1', lr=0.00003, results_df=results_lr, mean_reward=mean_reward, std_reward=std_reward) ",
   "id": "96edfa99efd6db49",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:03:35.127704Z",
     "start_time": "2024-11-17T07:03:35.114986Z"
    }
   },
   "cell_type": "code",
   "source": "results_lr",
   "id": "c80f6d265d49b920",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     Algorithm  Mean Reward  Standard Deviation  Learning Rate\n",
       "0  PPO_default   261.043689           41.428902        0.00030\n",
       "1   PPO_LR_x10   235.930474           83.436527        0.00300\n",
       "2  PPO_LR_x0.1   272.309767           22.056692        0.00003"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PPO_default</td>\n",
       "      <td>261.043689</td>\n",
       "      <td>41.428902</td>\n",
       "      <td>0.00030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PPO_LR_x10</td>\n",
       "      <td>235.930474</td>\n",
       "      <td>83.436527</td>\n",
       "      <td>0.00300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PPO_LR_x0.1</td>\n",
       "      <td>272.309767</td>\n",
       "      <td>22.056692</td>\n",
       "      <td>0.00003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 247
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Very stable agent, a bit better than the default one in terms of the reward",
   "id": "c9f8369dcf5b3572"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Another experiment: changing the architecture of the model",
   "id": "312a44dec5f5cdae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:10:01.173914Z",
     "start_time": "2024-11-16T12:56:19.411395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Increasing the amount of layers to 3 and neurons to 128\n",
    "\n",
    "policy_kwargs = dict(net_arch=[128, 128, 128])\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name='PPO_3_layer_128')"
   ],
   "id": "638513559dbbaf81",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x30acc8c20>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T13:11:48.386440Z",
     "start_time": "2024-11-16T13:11:48.309129Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_3_layer_128')",
   "id": "61d797efdea0d01c",
   "outputs": [],
   "execution_count": 196
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T14:29:35.027427Z",
     "start_time": "2024-11-16T14:28:57.872104Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent(model_path='models/PPO_3_layer_128')",
   "id": "5acd99558c0c1325",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 320.50967258667\n",
      "Episode: 2, Reward: 263.53044420536514\n",
      "Episode: 3, Reward: 253.91983949692423\n",
      "Episode: 4, Reward: 252.7634606809969\n",
      "Episode: 5, Reward: 268.32590577701535\n"
     ]
    }
   ],
   "execution_count": 199
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7b2c3a07d24a2ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T05:46:42.448371Z",
     "start_time": "2024-11-17T05:46:37.411619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## FINISH IT\n",
    "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_3_layer_128')"
   ],
   "id": "428c0e1260ed62f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 279.24673498 +/- 19.962474193817183\n"
     ]
    }
   ],
   "execution_count": 207
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T05:48:44.512088Z",
     "start_time": "2024-11-17T05:48:44.465235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_arch = pd.DataFrame(columns=['Algorithm', 'Mean Reward', 'Standard Deviation', 'Architecture'])\n",
    "\n",
    "results_arch.loc[0] = ['PPO_default', 266.884301, 21.949487, '[64, 64]']"
   ],
   "id": "a3b7c6897d0a1bd",
   "outputs": [],
   "execution_count": 208
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T05:48:51.374032Z",
     "start_time": "2024-11-17T05:48:51.355377Z"
    }
   },
   "cell_type": "code",
   "source": "results_arch = log_evaluation_arch(name='PPO 3-layered', arch='[128, 128, 128]', results_df=results_arch, mean_reward=mean_reward, std_reward=std_reward)",
   "id": "c4f2cba53dc27d60",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Let's play with it more",
   "id": "882c5cb1834500bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:50:39.203132Z",
     "start_time": "2024-11-17T06:39:13.984944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(net_arch=[128, 128])\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name='PPO_2_layer_128')"
   ],
   "id": "a3b5bb2e65a26c49",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x30ad60c80>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T20:00:08.705865Z",
     "start_time": "2024-11-14T20:00:08.644931Z"
    }
   },
   "cell_type": "code",
   "source": "model.policy",
   "id": "b07c2ea9ac1f73d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (value_net): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:51:29.501153Z",
     "start_time": "2024-11-17T06:51:29.447103Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_2_layer_128')",
   "id": "8c24f4705653bdd0",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:52:12.831808Z",
     "start_time": "2024-11-17T06:51:32.249280Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent(model_path='models/PPO_2_layer_128')",
   "id": "b3f6953b47984ee6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 247.20306813978377\n",
      "Episode: 2, Reward: 281.9463705270159\n",
      "Episode: 3, Reward: 324.4132904522078\n",
      "Episode: 4, Reward: 291.4043412706661\n",
      "Episode: 5, Reward: 296.50902780526906\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:52:32.876661Z",
     "start_time": "2024-11-17T06:52:27.465347Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_2_layer_128')",
   "id": "bb4bb3068e63344f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 275.68694847 +/- 26.82860779678868\n"
     ]
    }
   ],
   "execution_count": 232
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:56:42.380913Z",
     "start_time": "2024-11-17T06:56:42.370250Z"
    }
   },
   "cell_type": "code",
   "source": "results_arch = log_evaluation_arch(name='PPO 2-layered', arch='[128, 128]', results_df=results_arch, mean_reward=mean_reward, std_reward=std_reward)",
   "id": "ae0cfc42232a198d",
   "outputs": [],
   "execution_count": 241
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Making a policy function more complex by increasing the numbers of neurons twice in each layer\n",
    "\n",
    "Very successful model, low amount of useless moves"
   ],
   "id": "9244633231771911"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:32:55.809064Z",
     "start_time": "2024-11-16T17:19:54.780705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[64, 64]))\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name='PPO_2_layer_128_64')"
   ],
   "id": "35a656068890a695",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x300f5b710>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:32:55.909993Z",
     "start_time": "2024-11-16T17:32:55.901007Z"
    }
   },
   "cell_type": "code",
   "source": "model.policy",
   "id": "9ee18014efdb3125",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:32:56.081731Z",
     "start_time": "2024-11-16T17:32:56.051714Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_2_layer_128_64')",
   "id": "a2f2f6f4d8423da7",
   "outputs": [],
   "execution_count": 206
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T05:52:57.697434Z",
     "start_time": "2024-11-17T05:51:56.811542Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_2_layer_128_64')",
   "id": "82526fc557e0af4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 266.62139467787006\n",
      "Episode: 2, Reward: 273.6814372671445\n",
      "Episode: 3, Reward: 282.77987151886714\n",
      "Episode: 4, Reward: 265.1764128666954\n",
      "Episode: 5, Reward: 304.6802897341968\n"
     ]
    }
   ],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T05:53:27.881711Z",
     "start_time": "2024-11-17T05:53:20.608905Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_2_layer_128_64')",
   "id": "943997ca0934f6ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 257.00967935999995 +/- 25.95230886212065\n"
     ]
    }
   ],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T05:53:48.338765Z",
     "start_time": "2024-11-17T05:53:48.328432Z"
    }
   },
   "cell_type": "code",
   "source": "results_arch = log_evaluation_arch('PPO 2-layered 128/64', arch='Pi[128, 128], Vf[64, 64]', results_df=results_arch, mean_reward=mean_reward, std_reward=std_reward)",
   "id": "6bf8172557c57da7",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Making the value function more complex",
   "id": "373e64df47ed4bc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:07:31.910349Z",
     "start_time": "2024-11-17T05:55:34.087294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(net_arch=dict(pi=[64, 64], vf=[128, 128]))\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name='PPO_2_layer_64_128')"
   ],
   "id": "dd790c3cddbb4ff2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x300f596a0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:24:08.265560Z",
     "start_time": "2024-11-17T06:24:08.145773Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_2_layer_64_128')",
   "id": "e07d96a329a4c2a1",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:25:12.799328Z",
     "start_time": "2024-11-17T06:24:14.055427Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_2_layer_64_128')",
   "id": "e70a78da91d6db4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 243.15817723485623\n",
      "Episode: 2, Reward: 281.09322463872337\n",
      "Episode: 3, Reward: 291.40914278406206\n",
      "Episode: 4, Reward: 290.37787597840645\n",
      "Episode: 5, Reward: 244.91018521978071\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:26:29.947498Z",
     "start_time": "2024-11-17T06:26:23.034521Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_2_layer_64_128')",
   "id": "10f6062f323d895b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 244.96941969 +/- 46.34079186873261\n"
     ]
    }
   ],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T06:38:33.133194Z",
     "start_time": "2024-11-17T06:38:33.048410Z"
    }
   },
   "cell_type": "code",
   "source": "results_arch = log_evaluation_arch('PPO 2-layered 64/128', arch='Pi[64, 64], Vf[128,128]', results_df=results_arch, mean_reward=mean_reward, std_reward=std_reward)",
   "id": "fdb57892a4279638",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:04:34.241421Z",
     "start_time": "2024-11-17T07:04:34.230239Z"
    }
   },
   "cell_type": "code",
   "source": "results_arch",
   "id": "ecb399765ebb288b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              Algorithm  Mean Reward  Standard Deviation  \\\n",
       "0           PPO_default   261.043689           41.428902   \n",
       "1         PPO 3-layered   279.246735           19.962474   \n",
       "2  PPO 2-layered 128/64   257.009679           25.952309   \n",
       "3  PPO 2-layered 64/128   244.969420           46.340792   \n",
       "4         PPO 2-layered   275.686948           26.828608   \n",
       "\n",
       "               Architecture  \n",
       "0                  [64, 64]  \n",
       "1           [128, 128, 128]  \n",
       "2  Pi[128, 128], Vf[64, 64]  \n",
       "3   Pi[64, 64], Vf[128,128]  \n",
       "4                [128, 128]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Standard Deviation</th>\n",
       "      <th>Architecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PPO_default</td>\n",
       "      <td>261.043689</td>\n",
       "      <td>41.428902</td>\n",
       "      <td>[64, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PPO 3-layered</td>\n",
       "      <td>279.246735</td>\n",
       "      <td>19.962474</td>\n",
       "      <td>[128, 128, 128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PPO 2-layered 128/64</td>\n",
       "      <td>257.009679</td>\n",
       "      <td>25.952309</td>\n",
       "      <td>Pi[128, 128], Vf[64, 64]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PPO 2-layered 64/128</td>\n",
       "      <td>244.969420</td>\n",
       "      <td>46.340792</td>\n",
       "      <td>Pi[64, 64], Vf[128,128]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPO 2-layered</td>\n",
       "      <td>275.686948</td>\n",
       "      <td>26.828608</td>\n",
       "      <td>[128, 128]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Changing the activation function?",
   "id": "549b4f77b67dbc9d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:17:12.374826Z",
     "start_time": "2024-11-17T07:09:58.486348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch as th\n",
    "\n",
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[64, 64])\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=1_000_000, tb_log_name='PPO_ReLU')"
   ],
   "id": "40b2e3c1448b86c7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x306c55d60>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T07:59:20.878096Z",
     "start_time": "2024-11-17T07:59:20.749074Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_ReLU')",
   "id": "97eaafdbeb0c6644",
   "outputs": [],
   "execution_count": 251
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:00:02.626115Z",
     "start_time": "2024-11-17T07:59:27.853408Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_ReLU')",
   "id": "e256a766cf098d4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 284.62028903926443\n",
      "Episode: 2, Reward: 276.3256555585462\n",
      "Episode: 3, Reward: 271.42252015274016\n",
      "Episode: 4, Reward: 268.90678769184956\n",
      "Episode: 5, Reward: 269.53705624587974\n"
     ]
    }
   ],
   "execution_count": 252
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:00:26.045395Z",
     "start_time": "2024-11-17T08:00:21.302272Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_ReLU')",
   "id": "a4b02a59aaabf2fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 279.56737877 +/- 19.755591984854483\n"
     ]
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Making the 'perfect' model",
   "id": "5eeba436d584e958"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:29:08.027658Z",
     "start_time": "2024-11-17T08:03:24.722754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[128, 128, 128])\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, learning_rate=0.00003, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=2_000_000, tb_log_name='PPO_possibly_best')"
   ],
   "id": "badc0895a23e4ad4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x309180e00>"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 255
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:31:10.777150Z",
     "start_time": "2024-11-17T08:31:10.702607Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_possibly_best')",
   "id": "31382bf656f86750",
   "outputs": [],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T10:26:05.739089Z",
     "start_time": "2024-11-17T10:25:26.170604Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_possibly_best')",
   "id": "1d9449cc68198e10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 291.04192792533775\n",
      "Episode: 2, Reward: 254.48729856005542\n",
      "Episode: 3, Reward: 322.00864637392283\n",
      "Episode: 4, Reward: 295.51798064943887\n",
      "Episode: 5, Reward: 50.146137351463835\n"
     ]
    }
   ],
   "execution_count": 270
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:35:11.119688Z",
     "start_time": "2024-11-17T08:34:22.813968Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(algo=PPO, model_path='models/PPO_possibly_best', n_eval_episodes=1000)",
   "id": "ed4991f5ea8d530e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 270.819484418 +/- 43.2421512093431\n"
     ]
    }
   ],
   "execution_count": 263
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Making a LR default",
   "id": "83e2b597cf8e718c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:57:46.914618Z",
     "start_time": "2024-11-17T08:36:06.315074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=[128, 128, 128])\n",
    "\n",
    "env = make_vec_env(\"LunarLander-v2\", n_envs=8, vec_env_cls=SubprocVecEnv)\n",
    "model = PPO(\"MlpPolicy\", env, policy_kwargs=policy_kwargs, device=\"cpu\", tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=2_000_000, tb_log_name='PPO_2nd_possibly_best')"
   ],
   "id": "6b5d888a751f1c97",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x30916af60>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 264
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T08:59:12.522127Z",
     "start_time": "2024-11-17T08:59:12.461378Z"
    }
   },
   "cell_type": "code",
   "source": "model.save('models/PPO_2nd_possibly_best')",
   "id": "ea34073486bc3df6",
   "outputs": [],
   "execution_count": 266
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:34:52.178465Z",
     "start_time": "2024-11-18T02:34:16.215823Z"
    }
   },
   "cell_type": "code",
   "source": "visualize_agent('models/PPO_2nd_possibly_best')",
   "id": "c56d367cbc8f5238",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Reward: 296.19584472653185\n",
      "Episode: 2, Reward: 307.1673237229345\n",
      "Episode: 3, Reward: 297.9091820490997\n",
      "Episode: 4, Reward: 266.8851508065559\n",
      "Episode: 5, Reward: 284.9845581341762\n"
     ]
    }
   ],
   "execution_count": 275
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T02:35:55.277196Z",
     "start_time": "2024-11-18T02:35:12.517341Z"
    }
   },
   "cell_type": "code",
   "source": "mean_reward, std_reward = evaluate_rl_model(PPO, model_path='models/PPO_2nd_possibly_best', n_eval_episodes=1000)",
   "id": "3bd7c7f991932efc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO - Mean Reward: 280.86061939500007 +/- 37.38880397932653\n"
     ]
    }
   ],
   "execution_count": 276
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f8f917267c3a1c89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
